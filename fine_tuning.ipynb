{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renato-penna/fiap-tech-challenge-fase03/blob/main/fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a91371cc"
      },
      "source": [
        "### Mount Google Drive"
      ],
      "id": "a91371cc"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZeGuDQ7LEKc",
        "outputId": "bdde6b55-9c93-4799-9bca-661705fe187e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "3ZeGuDQ7LEKc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ed63a19"
      },
      "source": [
        "### Format Dataset"
      ],
      "id": "0ed63a19"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import load_dataset # Although load_dataset is imported, we'll process line by line for memory efficiency.\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/Fiap/trnTreaded.json\"\n",
        "OUTPUT_PATH_DATASET = \"/content/drive/MyDrive/Fiap/formatted_trn.json\"\n",
        "\n",
        "def format_dataset_into_model_input(data):\n",
        "    \"\"\"\n",
        "    Função ajustada para receber um dicionário completo (um item do dataset)\n",
        "    e extrair 'prompt' e 'completion' dele.\n",
        "    \"\"\"\n",
        "    prompt = data.get(\"prompt\", \"\")\n",
        "    completion = data.get(\"completion\", \"\")\n",
        "\n",
        "    instruction = \"Generate a description for the following item.\"\n",
        "\n",
        "    try:\n",
        "        # Extrai o texto entre \"Question:\" e \"Answer:\"\n",
        "        input_text = prompt.split(\"Question:\")[1].split(\"Answer:\")[0].strip()\n",
        "    except IndexError:\n",
        "        input_text = \"\"\n",
        "\n",
        "    # Extrai a resposta que vem depois de \"Answer:\"\n",
        "    try:\n",
        "        response = prompt.split(\"Answer:\")[1].strip()\n",
        "    except IndexError:\n",
        "        # Se 'Answer:' não estiver no prompt, usamos o campo 'completion'\n",
        "        response = completion.strip()\n",
        "\n",
        "    return instruction, input_text, response\n",
        "\n",
        "# Process the dataset line by line to avoid memory issues\n",
        "with open(DATA_PATH, 'r', encoding='utf-8') as input_file, \\\n",
        "     open(OUTPUT_PATH_DATASET, 'w', encoding='utf-8') as output_file:\n",
        "\n",
        "    for line in input_file:\n",
        "        try:\n",
        "            item = json.loads(line)\n",
        "            instruction, input_text, response = format_dataset_into_model_input(item)\n",
        "\n",
        "            formatted_item = {\n",
        "                \"instruction\": instruction,\n",
        "                \"input\": input_text,\n",
        "                \"output\": response\n",
        "            }\n",
        "\n",
        "            output_file.write(json.dumps(formatted_item, ensure_ascii=False) + '\\n')\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Skipping invalid JSON line: {line.strip()} - Error: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred processing line: {line.strip()} - Error: {e}\")\n",
        "\n",
        "print(f\"Dataset salvo em {OUTPUT_PATH_DATASET}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps41gITIlg26",
        "outputId": "462f4c4a-ace9-46ed-8092-7d8c67d725bf"
      },
      "id": "Ps41gITIlg26",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset salvo em /content/drive/MyDrive/Fiap/formatted_trn.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c77ca357"
      },
      "source": [
        "### Install Dependencies"
      ],
      "id": "c77ca357"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL3eUXTjVGjt"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install transformers datasets"
      ],
      "id": "dL3eUXTjVGjt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15318f72"
      },
      "source": [
        "### Setup and Imports"
      ],
      "id": "15318f72"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB2NJAU9VK1z"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "]"
      ],
      "id": "TB2NJAU9VK1z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87df9dfc"
      },
      "source": [
        "### Load Model and Tokenizer"
      ],
      "id": "87df9dfc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_986Mzoievms"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "id": "_986Mzoievms"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87061361"
      },
      "source": [
        "### Configure LoRA"
      ],
      "id": "87061361"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujR4qx-8e4NS"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "id": "ujR4qx-8e4NS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b54b5f0"
      },
      "source": [
        "### Format Prompts and Load Dataset"
      ],
      "id": "0b54b5f0"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "OUTPUT_PATH_DATASET = \"/content/drive/MyDrive/Fiap/formatted_trn.json\"\n",
        "\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=OUTPUT_PATH_DATASET, split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "yr5HMJI4w_8Y"
      },
      "id": "yr5HMJI4w_8Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d3e2e9e"
      },
      "source": [
        "### Setup SFTTrainer"
      ],
      "id": "7d3e2e9e"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "dfHQz-__xV6D"
      },
      "id": "dfHQz-__xV6D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11c28d63"
      },
      "source": [
        "### Train Model"
      ],
      "id": "11c28d63"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "du4M7VOnxwAm"
      },
      "id": "du4M7VOnxwAm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db86ab7e"
      },
      "source": [
        "### Run Inference"
      ],
      "id": "db86ab7e"
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Generate a description for the following item\",\n",
        "        \"California, The Beautiful Cookbook: Authentic Recipes from California\", # input\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wws8cYULzYYV",
        "outputId": "a42be9b1-126e-4174-8e66-22551c5586ef"
      },
      "id": "wws8cYULzYYV",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a description for the following item\n",
            "\n",
            "### Input:\n",
            "California, The Beautiful Cookbook: Authentic Recipes from California\n",
            "\n",
            "### Response:\n",
            "<|end_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb56cc82"
      },
      "source": [
        "### Save Model and Tokenizer"
      ],
      "id": "cb56cc82"
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Fiap/lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Fiap/lora_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mLna5_61D9M",
        "outputId": "f0ccd5ec-3867-455e-abbe-6072351ca82a"
      },
      "id": "7mLna5_61D9M",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Fiap/lora_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Fiap/lora_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Fiap/lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "https://github.com/renato-penna/fiap-tech-challenge-fase03/blob/main/fine_tuning.ipynb",
      "authorship_tag": "ABX9TyNP/spGgpBcK4aWcAVfFTxI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "nbformat": 4,
    "nbformat_minor": 0
  },
  "nbformat": 4,
  "nbformat_minor": 5
}