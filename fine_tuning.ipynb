{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/renato-penna/fiap-tech-challenge-fase03/blob/main/fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZeGuDQ7LEKc",
    "outputId": "3b7dfc87-10e1-41c9-d92f-4c6b3aef6be2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dL3eUXTjVGjt",
    "outputId": "13a5f02a-0022-4d8f-b89f-8f3d1af3f00e"
   },
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TB2NJAU9VK1z",
    "outputId": "e7c64efb-6517-4548-9821-750f94986952"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "DATA_PATH = \"/content/drive/MyDrive/Fiap/trnTreaded.json\"\n",
    "OUTPUT_PATH_DATASET = \"/content/drive/MyDrive/Fiap/formatted_trn.json\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "wfZ9FQs_bV43",
    "outputId": "2eec80f6-a83d-4617-8bac-28dba08bc0a4"
   },
   "outputs": [],
   "source": [
    "def format_dataset_into_model_input(data):\n",
    "\n",
    "    prompt = data.get(\"prompt\", \"\")\n",
    "    completion = data.get(\"completion\", \"\")\n",
    "\n",
    "    instruction = \"Generate a description for the following item.\"\n",
    "\n",
    "    try:\n",
    "        input_text = prompt.split(\"Question:\")[1].split(\"\\nAnswer:\")[0].strip()\n",
    "    except IndexError:\n",
    "\n",
    "        input_text = \"\"\n",
    "\n",
    "    response = completion.strip()\n",
    "\n",
    "    return instruction, input_text, response\n",
    "\n",
    "# Inicializando as listas para armazenar os dados\n",
    "instructions = []\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('json', data_files=DATA_PATH)\n",
    "\n",
    "# Processando o dataset\n",
    "for prompt in dataset['train']['input']:\n",
    "    instruction, input_text, response = format_dataset_into_model_input({\"prompt\": prompt})\n",
    "    instructions.append(instruction)\n",
    "    inputs.append(input_text)\n",
    "    outputs.append(response)\n",
    "\n",
    "# Criando o dicionÃ¡rio final\n",
    "formatted_data = {\n",
    "    \"instruction\": instructions,\n",
    "    \"input\": inputs,\n",
    "    \"output\": outputs\n",
    "}\n",
    "\n",
    "# Salvando o resultado em um arquivo JSON\n",
    "with open(OUTPUT_PATH_DATASET, 'w') as output_file:\n",
    "    json.dump(formatted_data, output_file, indent=4)\n",
    "\n",
    "print(f\"Dataset salvo em {OUTPUT_PATH_DATASET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_986Mzoievms"
   },
   "outputs": [],
   "source": [
    "format_dataset_into_model_input(data)\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujR4qx-8e4NS"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNtZo6LGBrTqiLK1WERccgQ",
   "gpuType": "T4",
   "include_colab_link": true,
   "mount_file_id": "1P6YEzNOgt7g9ZNH7il7490u8a22cPx0l",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "nbformat": 4,
  "nbformat_minor": 0
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
