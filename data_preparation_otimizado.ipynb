{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1 : Data Preparation Optimized\n",
        "\n",
        "**Problem:** Large JSON datasets (1GB+) cause memory crashes and the dataset can have many useless data we need remove or format to better results.\n",
        "\n",
        "**Solution:** Chunk processing + cleaning + formatting  \n",
        "\n",
        "**Objective:** Clean dataset ready for fine-tuning without crashes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instalations and Imports for necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ijson in ./.venv/lib/python3.13/site-packages (3.4.0)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (4.67.1)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (7.1.0)\n",
            "Requirement already satisfied: gdown in ./.venv/lib/python3.13/site-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.13/site-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in ./.venv/lib/python3.13/site-packages (from gdown) (2.32.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.13/site-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.13/site-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./.venv/lib/python3.13/site-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install ijson tqdm psutil gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in local environment\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import ijson\n",
        "import os\n",
        "import psutil\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import gdown\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    print(\"Running in Google Colab environment\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running in local environment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Dataset or Mount Google Drive and Define Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using local environment. Data will be stored in ./data/\n"
          ]
        }
      ],
      "source": [
        "# Download dataset from Google Drive\n",
        "DATASET_URL = \"https://drive.google.com/file/d/12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK/view\"\n",
        "FILE_ID = \"12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK\"\n",
        "\n",
        "# Define paths based on environment\n",
        "if IN_COLAB:\n",
        "    # Google Colab: Mount drive and use drive path\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Fiap\"\n",
        "    print(\"Google Drive mounted. Using Colab environment.\")\n",
        "else:\n",
        "    # Local: Use data folder\n",
        "    BASE_PATH = \"./data\"\n",
        "    # Create data directory if it doesn't exist\n",
        "    os.makedirs(BASE_PATH, exist_ok=True)\n",
        "    print(\"Using local environment. Data will be stored in ./data/\")\n",
        "\n",
        "\n",
        "# Processing pipeline files\n",
        "RAW_DATA_PATH = f\"{BASE_PATH}/trn.json\" # Original dataset\n",
        "CLEAN_DATA_PATH = f\"{BASE_PATH}/trn_cleaned.jsonl\" # Cleaned dataset\n",
        "FINAL_DATA_PATH = f\"{BASE_PATH}/trn_finetune.jsonl\" # Final dataset (Alpaca format)\n",
        "STATS_PATH = f\"{BASE_PATH}/processing_stats.json\" # Processing statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset already exists: ./data/trn.json\n",
            "Verifying JSON file format...\n",
            "Format: JSON Lines (JSONL) - each line is a JSON object\n",
            "\n",
            "Dataset ready for processing:\n",
            "  Size: 179.8 MB\n",
            "  Total lines: 1,305,265\n"
          ]
        }
      ],
      "source": [
        "from dataset_downloader import download_dataset, DatasetDownloader\n",
        "\n",
        "success = download_dataset(BASE_PATH, RAW_DATA_PATH, FILE_ID, DATASET_URL)\n",
        "if success:\n",
        "    downloader = DatasetDownloader(BASE_PATH, FILE_ID, DATASET_URL)\n",
        "    file_info = downloader.get_file_info(RAW_DATA_PATH)\n",
        "    print(f\"\\nDataset ready for processing:\")\n",
        "    print(f\"  Size: {file_info['size_mb']:.1f} MB\")\n",
        "    print(f\"  Total lines: {file_info['total_lines']:,}\")\n",
        "    TOTAL_LINES = file_info['total_lines']\n",
        "    DATASET_SIZE_MB = file_info['size_mb']\n",
        "else:\n",
        "    print(\"ERROR: Dataset download failed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Structure Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running comprehensive dataset analysis...\n",
            "Memory: 60.6% used (18.4GB/30.5GB)\n",
            "Counting lines in ./data/trn.json...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total lines: 1,305,265\n",
            "Analyzing dataset structure (50 samples)...\n",
            "Format: JSON Lines (JSONL) - Compatible\n",
            "\n",
            "=== DATASET ANALYSIS SUMMARY ===\n",
            "Sample size: 50\n",
            "Parse errors: 0\n",
            "Fields found: 5\n",
            "\n",
            "Field frequency:\n",
            "  - uid: 50/50 (100.0%)\n",
            "  - title: 50/50 (100.0%)\n",
            "  - content: 50/50 (100.0%)\n",
            "  - target_ind: 50/50 (100.0%)\n",
            "  - target_rel: 50/50 (100.0%)\n",
            "\n",
            "String field lengths:\n",
            "  - uid: avg=10, min=10, max=10\n",
            "  - title: avg=46, min=21, max=114\n",
            "  - content: avg=0, min=0, max=0\n",
            "\n",
            "Example record structure:\n",
            "  uid: 0000032050\n",
            "  title: Adult Ballet Tutu Purple\n",
            "  content: \n",
            "  target_ind: []\n",
            "  target_rel: []\n",
            "Counting lines in ./data/trn.json...\n",
            "Total lines: 1,305,265\n",
            "Memory: 60.8% used (18.5GB/30.5GB)\n",
            "Recommended chunk size: 200\n",
            "\n",
            "Recommended chunk size for processing: 200\n"
          ]
        }
      ],
      "source": [
        "# Import our dataset analyzer\n",
        "from dataset_analyzer import analyze_dataset, DatasetAnalyzer\n",
        "\n",
        "# Run complete dataset analysis\n",
        "print(\"Running comprehensive dataset analysis...\")\n",
        "analyzer = analyze_dataset(RAW_DATA_PATH, sample_size=50)\n",
        "\n",
        "# Get recommended settings for processing\n",
        "recommended_chunk_size = analyzer.get_recommended_chunk_size()\n",
        "print(f\"\\nRecommended chunk size for processing: {recommended_chunk_size}\")\n",
        "\n",
        "# Store results for next steps\n",
        "RECOMMENDED_CHUNK_SIZE = recommended_chunk_size"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
